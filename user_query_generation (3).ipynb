{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "c4bcba29-db60-4cd3-94d6-5b53142a50e0",
      "metadata": {
        "id": "c4bcba29-db60-4cd3-94d6-5b53142a50e0"
      },
      "source": [
        "# User Queries Generation Script"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d8f42f30-dca6-4ed7-a7f0-536a232e2280",
      "metadata": {
        "id": "d8f42f30-dca6-4ed7-a7f0-536a232e2280"
      },
      "source": [
        "This pipeline uses GPT-4o to:\n",
        "-\tDynamically categorize advertisements into domains and subdomains\n",
        "- Generate domain-specific “vague” user queries for each ad (LLM-style)\n",
        "- Maintain progress via checkpoints"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "71d97f07-3898-459e-b70c-4f32f3519df8",
      "metadata": {
        "id": "71d97f07-3898-459e-b70c-4f32f3519df8"
      },
      "source": [
        "### Files You Need"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7d330a3a-8cf7-4a0e-8977-d9c30ac036d1",
      "metadata": {
        "id": "7d330a3a-8cf7-4a0e-8977-d9c30ac036d1"
      },
      "source": [
        "| File                           | Purpose                                                                                 |\n",
        "|--------------------------------|-----------------------------------------------------------------------------------------|\n",
        "| `sampled_ads.csv`              | Input dataset containing a 4k-row random subset of `train_250k.tsv` ads. Used to generate LLM-style queries. |\n",
        "| `dynamic_queries_checkpoint.json` | Checkpoint file that stores processed ads along with their generated **vague** LLM-style queries and metadata (e.g., category, justification). Automatically updated after each ad is processed. |"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pipeline Steps\n",
        "1. Load input dataset\n",
        "`sampled_ads.csv` is loaded. A subset of 4000 rows is sampled randomly from `train_250k.tsv`\n",
        "\n",
        "2. Resume from Checkpoint (if exits):\n",
        "- Previously processed ads are loaded from `dynamic_queries_checkpoint.json`\n",
        "\n",
        "3. Iterate over ads:\n",
        "- Classify each ad into a known domain and subdomain.\n",
        "- Ask the LLM to generate a *vague* user query (LLM-style) for the ad.\n",
        "- Append results to `dynamic_queries_checkpoint.json`\n",
        "\n",
        "4. Rate limits:\n",
        "- Use `time.sleep(1)` to avoid hitting API rate limits."
      ],
      "metadata": {
        "id": "pN8f3hIXbfAP"
      },
      "id": "pN8f3hIXbfAP"
    },
    {
      "cell_type": "markdown",
      "id": "60266848-d1e3-4fd5-a0e1-aed6b2c6c06a",
      "metadata": {
        "id": "60266848-d1e3-4fd5-a0e1-aed6b2c6c06a"
      },
      "source": [
        "## Installation & Import"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "261786f4-9e72-4055-a075-e92721339bc7",
      "metadata": {
        "id": "261786f4-9e72-4055-a075-e92721339bc7",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "!pip install openai pandas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "574bbfb2-0d3d-4fcf-8822-59e15df531e1",
      "metadata": {
        "id": "574bbfb2-0d3d-4fcf-8822-59e15df531e1"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import openai\n",
        "import time\n",
        "import json\n",
        "from openai import OpenAI\n",
        "import os"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9a1fa77f-af04-4723-97dd-4a2b3c0a95de",
      "metadata": {
        "id": "9a1fa77f-af04-4723-97dd-4a2b3c0a95de"
      },
      "source": [
        "## User Query Generation Implementation"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## API Key\n",
        "This should be removed if you upload it to Github or somewhere."
      ],
      "metadata": {
        "id": "ctafaRC1dI2Z"
      },
      "id": "ctafaRC1dI2Z"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "93438e4b-eca3-42b4-b5a0-8a90f968f461",
      "metadata": {
        "id": "93438e4b-eca3-42b4-b5a0-8a90f968f461"
      },
      "outputs": [],
      "source": [
        "OPENAI_API_KEY = \"YOUR_KEY_HERE\""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Hidden cells\n",
        "This cells below are used to randomly sample the ad_dataset. This code also gives the original dataset column titles since it doesn't have any."
      ],
      "metadata": {
        "id": "Iz95PLO3cc0u"
      },
      "id": "Iz95PLO3cc0u"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "684e7e1e-e293-443e-bee8-41788e555ac2",
      "metadata": {
        "id": "684e7e1e-e293-443e-bee8-41788e555ac2"
      },
      "outputs": [],
      "source": [
        "# # Mount Google Drive\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')\n",
        "\n",
        "# # Define file path to the TSV\n",
        "# file_path = \"/content/drive/MyDrive/Algoverse/train_250k.tsv\"\n",
        "\n",
        "# #Load TSV safely, skipping bad lines\n",
        "# df = pd.read_csv(file_path, sep=\"\\t\", header=None, on_bad_lines='skip')\n",
        "\n",
        "# # Assign all 10 columns\n",
        "# df.columns = [\n",
        "#     \"product_id\", \"ad_id\", \"user_search_query\", \"ad_title\", \"ad_description\",\n",
        "#     \"url\", \"seller\", \"brand\", \"label\", \"image_id\"\n",
        "# ]\n",
        "\n",
        "\n",
        "# print(\"Loaded and cleaned. Shape:\", df.shape)\n",
        "# df.head()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # Sample ONCE and save for reuse\n",
        "# sampled_df = df.sample(n=4000, random_state=42).reset_index(drop=True)\n",
        "# sampled_df.to_csv(\"sampled_ads.csv\", index=False)  # Save it"
      ],
      "metadata": {
        "id": "T-Ou7uYMUYVw"
      },
      "id": "T-Ou7uYMUYVw",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Implementation"
      ],
      "metadata": {
        "id": "If05JYvHcsib"
      },
      "id": "If05JYvHcsib"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5afbc50a-a06e-40ae-8c7d-4c15687d7ad4",
      "metadata": {
        "scrolled": true,
        "id": "5afbc50a-a06e-40ae-8c7d-4c15687d7ad4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8d385210-ba6c-4657-b634-243a53570e9a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Resuming from checkpoint. 4000 ads completed.\n",
            "\n",
            " Done! 4000 ads saved to 'dynamic_queries_checkpoint.json'\n"
          ]
        }
      ],
      "source": [
        "client = OpenAI(api_key=OPENAI_API_KEY)\n",
        "\n",
        "# Load dataset\n",
        "csv_file = \"https://raw.githubusercontent.com/m1chae11u/llm-ad-integration/refs/heads/main/sampled_ads.csv\"\n",
        "df = pd.read_csv(csv_file)\n",
        "\n",
        "# File paths\n",
        "CHECKPOINT_FILE = \"dynamic_queries_checkpoint.json\"\n",
        "\n",
        "# Initialize or resume query checkpoint\n",
        "if os.path.exists(CHECKPOINT_FILE):\n",
        "    with open(CHECKPOINT_FILE, \"r\", encoding=\"utf-8\") as f:\n",
        "        query_data = json.load(f)\n",
        "    completed_indices = {entry[\"ad_index\"] for entry in query_data}\n",
        "    print(f\"Resuming from checkpoint. {len(query_data)} ads completed.\")\n",
        "else:\n",
        "    query_data = []\n",
        "    completed_indices = set()\n",
        "    print(\"Starting from scratch...\")\n",
        "\n",
        "# Main loop\n",
        "for idx, row in df.iterrows():\n",
        "    if idx in completed_indices:\n",
        "        continue\n",
        "\n",
        "    user_query = row[\"user_search_query\"]\n",
        "    ad_title = row[\"ad_title\"]\n",
        "    ad_description = row[\"ad_description\"]\n",
        "    url = row[\"url\"]\n",
        "    brand = row[\"brand\"]\n",
        "    ad_id = row[\"ad_id\"] if \"ad_id\" in row else None\n",
        "    prompt = f\"\"\"\n",
        "    You are an intelligent assistant helping with ad analysis.\n",
        "\n",
        "    Given the product ad information below, your task is to:\n",
        "    1. Identify a **broad domain** the ad belongs to (e.g., Electronics, Beauty & Personal Care).\n",
        "    2. Identify a **more specific subdomain** (e.g., Smartphones, Skincare).\n",
        "    3. Write a **vague, natural-sounding LLM-style user query** that someone might ask. This query should be broad enough to match the ad, but not directly name the brand or product.\n",
        "\n",
        "    Please label your output clearly:\n",
        "    DOMAIN: ...\n",
        "    SUBDOMAIN: ...\n",
        "    QUERY: ...\n",
        "\n",
        "    Example of an expected output:\n",
        "    DOMAIN: Beauty & Personal Care\n",
        "    SUBDOMAIN: Skincare\n",
        "    QUERY: What are some ways to even out my skin tone naturally?\n",
        "\n",
        "    Now do the same for the following ad:\n",
        "\n",
        "    Input Ad:\n",
        "    - Title: {ad_title}\n",
        "    - Description: {ad_description}\n",
        "    - URL: {url}\n",
        "    - Brand: {brand}\n",
        "\n",
        "    Respond only with the labeled output. No markdown, no explanations.\n",
        "    \"\"\"\n",
        "\n",
        "    try:\n",
        "        response = client.chat.completions.create(\n",
        "            model=\"gpt-4o\",\n",
        "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "            temperature=0.5,\n",
        "            max_tokens=300\n",
        "        )\n",
        "\n",
        "        content = response.choices[0].message.content.strip()\n",
        "\n",
        "        # Check if content is empty\n",
        "        if not content:\n",
        "            raise ValueError(f\"Empty response from GPT for ad #{idx}\")\n",
        "\n",
        "        # Strip any Markdown/code block formatting\n",
        "        if content.startswith(\"```\"):\n",
        "            content = content.split(\"```\")[-1].strip()\n",
        "\n",
        "        # Manually parse\n",
        "        lines = content.splitlines()\n",
        "        domain = next((line.split(\":\", 1)[1].strip() for line in lines if line.startswith(\"DOMAIN:\")), \"Other\")\n",
        "        subdomain = next((line.split(\":\", 1)[1].strip() for line in lines if line.startswith(\"SUBDOMAIN:\")), \"General\")\n",
        "        vague_query = next((line.split(\":\", 1)[1].strip() for line in lines if line.startswith(\"QUERY:\")), \"\")\n",
        "        print(f\"\\nAd #{idx} | Domain: {domain} | Subdomain: {subdomain}\")\n",
        "        print(f\"Query: {vague_query}\")\n",
        "\n",
        "\n",
        "        # Save query data\n",
        "        query_data.append({\n",
        "            \"ad_index\": idx,\n",
        "            \"ad_id\": ad_id,\n",
        "            \"ad_product\": ad_title,\n",
        "            \"domain\": domain,\n",
        "            \"subdomain\": subdomain,\n",
        "            \"vague_query\": vague_query\n",
        "        })\n",
        "\n",
        "        with open(CHECKPOINT_FILE, \"w\", encoding=\"utf-8\") as f:\n",
        "            json.dump(query_data, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error on ad #{idx}: {e}\")\n",
        "        time.sleep(5)\n",
        "        continue\n",
        "\n",
        "    time.sleep(1)  # Rate limiting\n",
        "\n",
        "# Wrap up\n",
        "print(f\"\\n Done! {len(query_data)} ads saved to '{CHECKPOINT_FILE}'\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5bcafbd1-22cd-4fa0-96f0-f9f033c92991",
      "metadata": {
        "id": "5bcafbd1-22cd-4fa0-96f0-f9f033c92991",
        "outputId": "517554bf-535b-4458-f873-057464aaa39a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved 4000 queries to user_queries.json\n"
          ]
        }
      ],
      "source": [
        "# Save to file\n",
        "with open(\"user_queries.json\", \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(query_data, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "print(f\"Saved {len(query_data)} queries to user_queries.json\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}