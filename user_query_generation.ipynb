{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "c4bcba29-db60-4cd3-94d6-5b53142a50e0",
      "metadata": {
        "id": "c4bcba29-db60-4cd3-94d6-5b53142a50e0"
      },
      "source": [
        "# User Queries Generation Script"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d8f42f30-dca6-4ed7-a7f0-536a232e2280",
      "metadata": {
        "id": "d8f42f30-dca6-4ed7-a7f0-536a232e2280"
      },
      "source": [
        "This pipeline uses GPT-4o to:\n",
        "-\tDynamically categorize advertisements into domains\n",
        "- Generate domain-specific “vague” user queries for each ad (LLM-style)\n",
        "- Maintain progress via checkpoints\n",
        "- Expand the category list if new domains emerge\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "71d97f07-3898-459e-b70c-4f32f3519df8",
      "metadata": {
        "id": "71d97f07-3898-459e-b70c-4f32f3519df8"
      },
      "source": [
        "### Files You Need"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7d330a3a-8cf7-4a0e-8977-d9c30ac036d1",
      "metadata": {
        "id": "7d330a3a-8cf7-4a0e-8977-d9c30ac036d1"
      },
      "source": [
        "| File                           | Purpose                                                                                 |\n",
        "|--------------------------------|-----------------------------------------------------------------------------------------|\n",
        "| `sampled_ads.csv`              | Input dataset containing a 4k-row random subset of `train_250k.tsv` ads. Used to generate LLM-style queries. |\n",
        "| `dynamic_queries_checkpoint.json` | Checkpoint file that stores processed ads along with their generated **vague** LLM-style queries and metadata (e.g., category, justification). Automatically updated after each ad is processed. |\n",
        "| `dynamic_category_list.json`   | Stores the list of ad domain categories. Dynamically updated if the LLM proposes a new category during classification. |"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pipeline Steps\n",
        "1. Load input dataset\n",
        "`sampled_ads.csv` is loaded. A subset of 4000 rows is sampled randomly from `train_250k.tsv`\n",
        "\n",
        "2. Resume from Checkpoint (if exits):\n",
        "- Previously processed ads are loaded from `dynamic_queries_checkpoint.json`\n",
        "\n",
        "3. Iterate over ads:\n",
        "- Classify each ad into a known domain or propose a new one via LLM (with reasoning).\n",
        "- Update and save `dynamic_category_list.json` if a new domain is added.\n",
        "- Ask the LLM to generate a *vague* user query (LLM-style) for the ad.\n",
        "- Append results to `dynamic_queries_checkpoint.json`\n",
        "\n",
        "4. Rate limits:\n",
        "- Use `time.sleep(1)` to avoid hitting API rate limits."
      ],
      "metadata": {
        "id": "pN8f3hIXbfAP"
      },
      "id": "pN8f3hIXbfAP"
    },
    {
      "cell_type": "markdown",
      "id": "60266848-d1e3-4fd5-a0e1-aed6b2c6c06a",
      "metadata": {
        "id": "60266848-d1e3-4fd5-a0e1-aed6b2c6c06a"
      },
      "source": [
        "## Installation & Import"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "261786f4-9e72-4055-a075-e92721339bc7",
      "metadata": {
        "id": "261786f4-9e72-4055-a075-e92721339bc7"
      },
      "outputs": [],
      "source": [
        "!pip install openai pandas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "574bbfb2-0d3d-4fcf-8822-59e15df531e1",
      "metadata": {
        "id": "574bbfb2-0d3d-4fcf-8822-59e15df531e1"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import openai\n",
        "import time\n",
        "import json\n",
        "from openai import OpenAI\n",
        "import os"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9a1fa77f-af04-4723-97dd-4a2b3c0a95de",
      "metadata": {
        "id": "9a1fa77f-af04-4723-97dd-4a2b3c0a95de"
      },
      "source": [
        "## User Query Generation Implementation"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## API Key\n",
        "This should be removed if you upload it to Github or somewhere."
      ],
      "metadata": {
        "id": "ctafaRC1dI2Z"
      },
      "id": "ctafaRC1dI2Z"
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "93438e4b-eca3-42b4-b5a0-8a90f968f461",
      "metadata": {
        "id": "93438e4b-eca3-42b4-b5a0-8a90f968f461"
      },
      "outputs": [],
      "source": [
        "OPENAI_API_KEY = \"Enter your code here\""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Hidden cells\n",
        "This cells below are used to randomly sample the ad_dataset. This code also gives the original dataset column titles since it doesn't have any."
      ],
      "metadata": {
        "id": "Iz95PLO3cc0u"
      },
      "id": "Iz95PLO3cc0u"
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "684e7e1e-e293-443e-bee8-41788e555ac2",
      "metadata": {
        "id": "684e7e1e-e293-443e-bee8-41788e555ac2"
      },
      "outputs": [],
      "source": [
        "# # Mount Google Drive\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')\n",
        "\n",
        "# # Define file path to the TSV\n",
        "# file_path = \"/content/drive/MyDrive/Algoverse/train_250k.tsv\"\n",
        "\n",
        "# #Load TSV safely, skipping bad lines\n",
        "# df = pd.read_csv(file_path, sep=\"\\t\", header=None, on_bad_lines='skip')\n",
        "\n",
        "# # Assign all 10 columns\n",
        "# df.columns = [\n",
        "#     \"product_id\", \"ad_id\", \"user_search_query\", \"ad_title\", \"ad_description\",\n",
        "#     \"url\", \"seller\", \"brand\", \"label\", \"image_id\"\n",
        "# ]\n",
        "\n",
        "\n",
        "# print(\"Loaded and cleaned. Shape:\", df.shape)\n",
        "# df.head()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # Sample ONCE and save for reuse\n",
        "# sampled_df = df.sample(n=4000, random_state=42).reset_index(drop=True)\n",
        "# sampled_df.to_csv(\"sampled_ads.csv\", index=False)  # Save it"
      ],
      "metadata": {
        "id": "T-Ou7uYMUYVw"
      },
      "id": "T-Ou7uYMUYVw",
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Implementation"
      ],
      "metadata": {
        "id": "If05JYvHcsib"
      },
      "id": "If05JYvHcsib"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5afbc50a-a06e-40ae-8c7d-4c15687d7ad4",
      "metadata": {
        "scrolled": true,
        "id": "5afbc50a-a06e-40ae-8c7d-4c15687d7ad4"
      },
      "outputs": [],
      "source": [
        "client = OpenAI(api_key=OPENAI_API_KEY)\n",
        "\n",
        "# Load dataset\n",
        "csv_file = \"https://raw.githubusercontent.com/m1chae11u/llm-ad-integration/refs/heads/main/sampled_ads.csv\"\n",
        "df = pd.read_csv(csv_file)\n",
        "\n",
        "# Checkpoint and category files\n",
        "CHECKPOINT_FILE = \"dynamic_queries_checkpoint.json\"\n",
        "CATEGORY_FILE = \"dynamic_category_list.json\"\n",
        "\n",
        "# Load or initialize category list\n",
        "if os.path.exists(CATEGORY_FILE):\n",
        "    with open(CATEGORY_FILE, \"r\", encoding=\"utf-8\") as f:\n",
        "        domain_list = json.load(f)\n",
        "else:\n",
        "    domain_list = [\n",
        "        \"Electronics\", \"Apparel & Fashion\", \"Beauty & Personal Care\",\n",
        "        \"Home & Living\", \"Travel\", \"Tools & Hardware\",\n",
        "        \"Health & Wellness\", \"Automotive\"\n",
        "    ]\n",
        "\n",
        "# Load checkpoint if exists\n",
        "if os.path.exists(CHECKPOINT_FILE):\n",
        "    with open(CHECKPOINT_FILE, \"r\", encoding=\"utf-8\") as f:\n",
        "        query_data = json.load(f)\n",
        "    completed_indices = {entry[\"ad_index\"] for entry in query_data}\n",
        "    print(f\"Resuming from checkpoint. {len(query_data)} ads completed.\")\n",
        "else:\n",
        "    query_data = []\n",
        "    completed_indices = set()\n",
        "    print(\"Starting from scratch...\")\n",
        "\n",
        "# Main loop\n",
        "for idx, row in df.iterrows():\n",
        "    if idx in completed_indices:\n",
        "        continue\n",
        "\n",
        "    query = row[\"user_search_query\"]\n",
        "    title = row[\"ad_title\"]\n",
        "    description = row[\"ad_description\"]\n",
        "    url = row[\"url\"]\n",
        "    brand = row[\"brand\"]\n",
        "\n",
        "    # Step 1: Classify ad domain\n",
        "    domain_prompt = f\"\"\"\n",
        "      You are categorizing product ads into high-level domains.\n",
        "\n",
        "      Here are the current domain categories:\n",
        "      {', '.join(domain_list)}\n",
        "\n",
        "      Please classify the following ad into one of the above domains.\n",
        "      If none apply, propose a new domain and explain why.\n",
        "\n",
        "      Ad Title: {title}\n",
        "      Ad Description: {description}\n",
        "      Brand: {brand}\n",
        "\n",
        "      Return this JSON format:\n",
        "      {{\n",
        "        \"domain\": \"category name\",\n",
        "        \"is_new\": true or false,\n",
        "        \"justification\": \"brief explanation\"\n",
        "      }}\n",
        "      \"\"\"\n",
        "\n",
        "    try:\n",
        "        response = client.chat.completions.create(\n",
        "            model=\"gpt-4o\",\n",
        "            messages=[{\"role\": \"user\", \"content\": domain_prompt}],\n",
        "            temperature=0.3,\n",
        "            max_tokens=150\n",
        "        )\n",
        "\n",
        "        content = response.choices[0].message.content.strip()\n",
        "        if not content:\n",
        "            raise ValueError(\"Empty response from GPT\")\n",
        "\n",
        "        if content.startswith(\"```json\"):\n",
        "            content = content.replace(\"```json\", \"\").strip()\n",
        "        if content.endswith(\"```\"):\n",
        "            content = content[:-3].strip()\n",
        "\n",
        "        try:\n",
        "            domain_result = json.loads(content)\n",
        "        except json.JSONDecodeError:\n",
        "            print(f\"Malformed JSON in domain response for ad #{idx}.\\nRaw content:\\n{content}\\n\")\n",
        "            continue\n",
        "\n",
        "        category = domain_result.get(\"domain\", \"Other\")\n",
        "        is_new = domain_result.get(\"is_new\", False)\n",
        "        justification = domain_result.get(\"justification\", \"\")\n",
        "\n",
        "        if is_new and category not in domain_list:\n",
        "            domain_list.append(category)\n",
        "            print(f\"New category added: {category} — {justification}\")\n",
        "            with open(CATEGORY_FILE, \"w\", encoding=\"utf-8\") as f:\n",
        "                json.dump(domain_list, f, indent=2)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error on domain classification for ad #{idx}: {e}\")\n",
        "        continue\n",
        "\n",
        "    # Step 2: Generate only the vague query\n",
        "    query_prompt = f\"\"\"\n",
        "      You're helping us test an AI advertisement system.\n",
        "\n",
        "      Here is the information about the product:\n",
        "      - User Query (from a traditional search engine): {query}\n",
        "      - Ad Title: {title}\n",
        "      - Description: {description}\n",
        "      - Domain: {url}\n",
        "      - Brand: {brand}\n",
        "      - Category: {category}\n",
        "\n",
        "      Now, return only a vague user query that would be appropriate in the context of a user interacting with a large language model (LLM), rather than a traditional search engine.\n",
        "\n",
        "      Respond with just the vague query as a plain string. Do not return JSON, markdown, or explanations.\n",
        "    \"\"\"\n",
        "\n",
        "    try:\n",
        "        response = client.chat.completions.create(\n",
        "            model=\"gpt-4o\",\n",
        "            messages=[{\"role\": \"user\", \"content\": query_prompt}],\n",
        "            temperature=0.7,\n",
        "            max_tokens=600\n",
        "        )\n",
        "\n",
        "        query = response.choices[0].message.content.strip().strip('\"')\n",
        "\n",
        "        print(f\"\\nAd #{idx} | Ad Title: {title} | Category: {category}\")\n",
        "        print(f\"Query: {query}\")\n",
        "\n",
        "        # Save progress\n",
        "        query_data.append({\n",
        "            \"ad_index\": idx,\n",
        "            \"ad_product\": title,\n",
        "            \"category\": category,\n",
        "            \"is_new_category\": is_new,\n",
        "            \"justification\": justification,\n",
        "            \"llm_queries\": query,\n",
        "        })\n",
        "\n",
        "        with open(CHECKPOINT_FILE, \"w\", encoding=\"utf-8\") as f:\n",
        "            json.dump(query_data, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error generating queries on ad #{idx}: {e}\")\n",
        "        time.sleep(5)\n",
        "        continue\n",
        "\n",
        "    time.sleep(1)\n",
        "\n",
        "# Done generating\n",
        "print(f\"\\nAll done! {len(query_data)} entries saved to '{CHECKPOINT_FILE}'\")\n",
        "print(f\"Final domain list saved to '{CATEGORY_FILE}'\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5bcafbd1-22cd-4fa0-96f0-f9f033c92991",
      "metadata": {
        "id": "5bcafbd1-22cd-4fa0-96f0-f9f033c92991"
      },
      "outputs": [],
      "source": [
        "# Save to file\n",
        "with open(\"user_queries.json\", \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(query_data, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "print(f\"Saved {len(query_data)} queries to user_queries.json\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
