{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline vs Optimized Model Comparison\n",
    "\n",
    "This notebook compares the performance of a baseline (untrained) model against our optimized PPO model for ad integration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'judge'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 14\u001b[39m\n\u001b[32m     11\u001b[39m \u001b[38;5;66;03m# Add src to path\u001b[39;00m\n\u001b[32m     12\u001b[39m sys.path.append(os.path.abspath(\u001b[33m'\u001b[39m\u001b[33m..\u001b[39m\u001b[33m'\u001b[39m))\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msrc\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mjudge\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcoherence\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m judge_coherence\n\u001b[32m     15\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msrc\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mjudge\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mhelpfulness\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m judge_helpfulness\n\u001b[32m     16\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msrc\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mjudge\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msalience\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m judge_ad_salience\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/School/algoverse/llm-ad-integration/src/judge/__init__.py:1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcoherence\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m judge_coherence\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mhelpfulness\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m judge_helpfulness\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01msalience\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m judge_ad_salience\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/School/algoverse/llm-ad-integration/src/judge/coherence.py:1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mjudge\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m call_deepseek_and_extract_json\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mjudge_coherence\u001b[39m(response, query):\n\u001b[32m      4\u001b[39m     prompt = \u001b[33mf\u001b[39m\u001b[33m\"\"\"\u001b[39m\n\u001b[32m      5\u001b[39m \u001b[33mSYSTEM\u001b[39m\n\u001b[32m      6\u001b[39m \u001b[33mYou are an impartial evaluator.\u001b[39m\n\u001b[32m   (...)\u001b[39m\u001b[32m     49\u001b[39m \u001b[38;5;132;01m{\u001b[39;00mresponse\u001b[38;5;132;01m}\u001b[39;00m\n\u001b[32m     50\u001b[39m \u001b[33m\"\"\"\u001b[39m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'judge'"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from rich.console import Console\n",
    "from rich.table import Table\n",
    "from rich.text import Text\n",
    "from rich.padding import Padding\n",
    "\n",
    "# Add src to path\n",
    "sys.path.append(os.path.abspath('..'))\n",
    "\n",
    "from src.judge.coherence import judge_coherence\n",
    "from src.judge.helpfulness import judge_helpfulness\n",
    "from src.judge.salience import judge_ad_salience\n",
    "from src.judge.detectability import judge_detectability\n",
    "from src.generate.generator import generate_responses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load test data\n",
    "df = pd.read_csv(\"../data/merged_queries_ads.csv\")\n",
    "print(f\"Loaded {len(df)} test examples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ad_facts(entry):\n",
    "    return {\n",
    "        'ad_product': entry['ad_product'],\n",
    "        'brand': entry['brand'],\n",
    "        'url': entry['url'],\n",
    "        'description': entry['ad_description']\n",
    "    }\n",
    "\n",
    "def evaluate_response(query, response_with_ad, response_without_ad, ad_facts):\n",
    "    \"\"\"Evaluate a response using all judge metrics\"\"\"\n",
    "    results = {}\n",
    "    \n",
    "    # Coherence\n",
    "    coherence = judge_coherence(response_with_ad, query)\n",
    "    results['coherence'] = coherence\n",
    "    \n",
    "    # Helpfulness\n",
    "    helpfulness = judge_helpfulness(query, response_with_ad)\n",
    "    results['helpfulness'] = helpfulness\n",
    "    \n",
    "    # Ad Salience\n",
    "    ad_salience = judge_ad_salience(query, response_with_ad, ad_facts)\n",
    "    results['ad_salience'] = ad_salience\n",
    "    \n",
    "    # Detectability\n",
    "    detectability = judge_detectability(response_with_ad, response_without_ad)\n",
    "    results['detectability'] = detectability\n",
    "    \n",
    "    return results\n",
    "\n",
    "def print_comparison(baseline_results, optimized_results, query, ad_facts):\n",
    "    \"\"\"Print a comparison of baseline vs optimized results\"\"\"\n",
    "    console = Console()\n",
    "    \n",
    "    console.rule(\"[bold blue]Query and Ad Info\")\n",
    "    console.print(f\"[bold yellow]Query:[/bold yellow] {query}\")\n",
    "    console.print(f\"[bold yellow]Ad Product:[/bold yellow] {ad_facts['ad_product']}\")\n",
    "    console.print(f\"[bold yellow]Brand:[/bold yellow] {ad_facts['brand']}\")\n",
    "    \n",
    "    # Create comparison table\n",
    "    table = Table(show_header=True, header_style=\"bold magenta\")\n",
    "    table.add_column(\"Metric\")\n",
    "    table.add_column(\"Baseline\")\n",
    "    table.add_column(\"Optimized\")\n",
    "    table.add_column(\"Improvement\")\n",
    "    \n",
    "    # Add rows for each metric\n",
    "    metrics = {\n",
    "        \"Coherence\": (\"coherence\", \"Coherence Score\"),\n",
    "        \"Helpfulness\": (\"helpfulness\", \"Helpfulness Score\"),\n",
    "        \"Ad Salience\": (\"ad_salience\", \"Ad Salience Score\"),\n",
    "        \"Detectability\": (\"detectability\", \"detectability_cosine\")\n",
    "    }\n",
    "    \n",
    "    for metric_name, (result_key, score_key) in metrics.items():\n",
    "        baseline_score = baseline_results[result_key].get(score_key, 0)\n",
    "        optimized_score = optimized_results[result_key].get(score_key, 0)\n",
    "        improvement = optimized_score - baseline_score\n",
    "        \n",
    "        table.add_row(\n",
    "            metric_name,\n",
    "            f\"{baseline_score:.2f}\",\n",
    "            f\"{optimized_score:.2f}\",\n",
    "            f\"[green]+{improvement:.2f}[/green]\" if improvement > 0 else f\"[red]{improvement:.2f}[/red]\"\n",
    "        )\n",
    "    \n",
    "    console.print(table)\n",
    "    console.rule(\"[bold blue]End of Comparison\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of examples to evaluate\n",
    "NUM_EXAMPLES = 5\n",
    "\n",
    "# Store results\n",
    "comparison_results = []\n",
    "\n",
    "for idx, row in tqdm(df.head(NUM_EXAMPLES).iterrows(), total=NUM_EXAMPLES):\n",
    "    query = row['vague_query']\n",
    "    ad_facts = get_ad_facts(row)\n",
    "    \n",
    "    # Generate responses\n",
    "    baseline_without_ad, baseline_with_ad = generate_responses(query, ad_facts, use_optimized=False)\n",
    "    optimized_without_ad, optimized_with_ad = generate_responses(query, ad_facts, use_optimized=True)\n",
    "    \n",
    "    # Evaluate responses\n",
    "    baseline_results = evaluate_response(query, baseline_with_ad, baseline_without_ad, ad_facts)\n",
    "    optimized_results = evaluate_response(query, optimized_with_ad, optimized_without_ad, ad_facts)\n",
    "    \n",
    "    # Print comparison\n",
    "    print(f\"\\nEvaluating example {idx + 1}/{NUM_EXAMPLES}\")\n",
    "    print_comparison(baseline_results, optimized_results, query, ad_facts)\n",
    "    \n",
    "    # Store results\n",
    "    comparison_results.append({\n",
    "        'query': query,\n",
    "        'ad_facts': ad_facts,\n",
    "        'baseline_results': baseline_results,\n",
    "        'optimized_results': optimized_results\n",
    "    })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aggregate Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_average_scores(results, model_type):\n",
    "    \"\"\"Calculate average scores for a model type\"\"\"\n",
    "    metrics = {\n",
    "        \"Coherence\": (\"coherence\", \"Coherence Score\"),\n",
    "        \"Helpfulness\": (\"helpfulness\", \"Helpfulness Score\"),\n",
    "        \"Ad Salience\": (\"ad_salience\", \"Ad Salience Score\"),\n",
    "        \"Detectability\": (\"detectability\", \"detectability_cosine\")\n",
    "    }\n",
    "    \n",
    "    scores = {}\n",
    "    for metric_name, (result_key, score_key) in metrics.items():\n",
    "        values = [r[f'{model_type}_results'][result_key].get(score_key, 0) for r in results]\n",
    "        scores[metric_name] = sum(values) / len(values)\n",
    "    \n",
    "    return scores\n",
    "\n",
    "# Calculate averages\n",
    "baseline_avg = calculate_average_scores(comparison_results, 'baseline')\n",
    "optimized_avg = calculate_average_scores(comparison_results, 'optimized')\n",
    "\n",
    "# Print summary\n",
    "console = Console()\n",
    "console.rule(\"[bold blue]Overall Results Summary\")\n",
    "\n",
    "table = Table(show_header=True, header_style=\"bold magenta\")\n",
    "table.add_column(\"Metric\")\n",
    "table.add_column(\"Baseline Avg\")\n",
    "table.add_column(\"Optimized Avg\")\n",
    "table.add_column(\"Improvement\")\n",
    "\n",
    "for metric in baseline_avg.keys():\n",
    "    improvement = optimized_avg[metric] - baseline_avg[metric]\n",
    "    table.add_row(\n",
    "        metric,\n",
    "        f\"{baseline_avg[metric]:.2f}\",\n",
    "        f\"{optimized_avg[metric]:.2f}\",\n",
    "        f\"[green]+{improvement:.2f}[/green]\" if improvement > 0 else f\"[red]{improvement:.2f}[/red]\"\n",
    "    )\n",
    "\n",
    "console.print(table)\n",
    "console.rule(\"[bold blue]End of Summary\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
