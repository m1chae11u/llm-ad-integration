import llamafactory.train.ppo.workflow as _wf
_wf.create_reward_model = lambda model, model_args, finetuning_args: None
from llamafactory.train.ppo.workflow import CustomPPOTrainer, run_ppo
import torch
import asyncio
from typing import List
from llamafactory.hparams import (
    ModelArguments,
    DataArguments,
    TrainingArguments,
    FinetuningArguments,
    GeneratingArguments,
)
from datasets import load_dataset
from transformers import AutoTokenizer, BitsAndBytesConfig
from trl import AutoModelForCausalLMWithValueHead, create_reference_model
from .prompts import get_prompt_with_ad, get_prompt_without_ad
from judge import (
    judge_coherence_async,
    judge_helpfulness_async,
    judge_ad_salience_async,
)
from judge.detectability import judge_detectability_async
import pandas as pd
from pathlib import Path
import sys  # for handling interrupt and exit
from config import DATA_FILE
# Suppress deprecation warnings about Trainer.tokenizer
import warnings
warnings.filterwarnings(
    'ignore',
    'Trainer\\.tokenizer is now deprecated.*',
    category=UserWarning,
)
import logging
logging.basicConfig(format='%(asctime)s - %(levelname)s - %(message)s', level=logging.INFO)
logger = logging.getLogger(__name__)
# Global list to collect PPO judging logs
TRAINING_LOGS: List[dict] = []
# Buffer to store no-ad responses generated by PPO for detectability
PPO_NO_AD_BUFFER: List[str] = []

DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# Define a custom collator to pad and retain metadata
class PPODataCollator:
    def __init__(self, tokenizer, ad_keys=None):
        self.tokenizer = tokenizer
        self.ad_keys = ad_keys or ["ad_product", "brand", "url", "ad_description"]

    def __call__(self, features):
        # features: list of dicts with keys: input_ids, attention_mask, and raw fields
        batch = self.tokenizer.pad(
            features,
            return_tensors="pt"
        )
        # retain metadata for detectability and ad alignment
        batch_size = len(features)
        # idx is positional index within this batch
        batch["idx"] = torch.arange(batch_size, dtype=torch.long)
        # include ad fields
        for key in self.ad_keys:
            batch[key] = [f.get(key) for f in features]
        return batch

class MyPPOTrainer(CustomPPOTrainer):
    def __init__(
        self,
        *args,
        ad_facts_list: List[dict],
        **kwargs,
    ):
        super().__init__(*args, **kwargs)
        self.ad_facts_list = ad_facts_list
        # Store no-ad responses for detectability
        self.no_ad_responses: List[str] = []

    @torch.no_grad()
    def get_inputs(self, batch):
        # Decode queries from token IDs
        raw_qs = [
            self.tokenizer.decode(ids, skip_special_tokens=True)
            for ids in batch["input_ids"]
        ]
        # Extract ad metadata directly from batch (no cursor needed)
        ad_products = batch["ad_product"]
        brands = batch["brand"]
        urls = batch["url"]
        descriptions = batch["ad_description"]

        # Prepare both with-ad and without-ad prompts
        full_prompts_ad = []
        full_prompts_no_ad = []
        for i, q in enumerate(raw_qs):
            # Build full ad text block for with-ad prompt using batch metadata
            ad_text = (
                f"Product: {ad_products[i]}\n"
                f"Brand:   {brands[i]}\n"
                f"URL:     {urls[i]}\n"
                f"Desc:    {descriptions[i]}"
            )
            full_prompts_ad.append(get_prompt_with_ad(q, ad_text))
            # Also prepare no-ad prompt for detectability
            full_prompts_no_ad.append(get_prompt_without_ad(q))

        tok_ad = self.tokenizer(
            full_prompts_ad,
            return_tensors="pt",
            padding=True,
            truncation=True,
            max_length=self.tokenizer.model_max_length,
        )
        input_ids = tok_ad.input_ids.to(self.current_device)
        attention_mask = tok_ad.attention_mask.to(self.current_device)

        # Tokenize no-ad prompts
        tok_no_ad = self.tokenizer(
            full_prompts_no_ad,
            return_tensors="pt",
            padding=True,
            truncation=True,
            max_length=self.tokenizer.model_max_length,
        )
        input_ids_no_ad = tok_no_ad.input_ids.to(self.current_device)
        attention_mask_no_ad = tok_no_ad.attention_mask.to(self.current_device)

        model = self.accelerator.unwrap_model(self.model)
        # Generate with-ad responses
        logger.info(f"🟨 Generating with-ad responses for batch of size {len(full_prompts_ad)}...")
        # Filter out unsupported or default kwargs before generation
        gen_kwargs = self.generation_config.to_dict().copy()
        gen_kwargs.pop("skip_special_tokens", None)
        gen_kwargs.pop("max_length", None)
        gen_kwargs = {"input_ids": input_ids, "attention_mask": attention_mask, **gen_kwargs}
        gen_out_ad = model.generate(**gen_kwargs)
        logger.info("✅ With-ad generation complete.")
        # Generate no-ad responses for detectability
        logger.info(f"🟦 Generating no-ad responses for batch of size {len(full_prompts_no_ad)}...")
        # Filter out unsupported or default kwargs before generation
        gen_kwargs_no_ad = self.generation_config.to_dict().copy()
        gen_kwargs_no_ad.pop("skip_special_tokens", None)
        gen_kwargs_no_ad.pop("max_length", None)
        gen_kwargs_no_ad = {"input_ids": input_ids_no_ad, "attention_mask": attention_mask_no_ad, **gen_kwargs_no_ad}
        gen_out_no_ad = model.generate(**gen_kwargs_no_ad)
        logger.info("✅ No-ad generation complete.")

        # Extract response tensors for both with-ad and no-ad parallelly
        response_tensors = []
        response_tensors_no_ad = []
        batch_size = gen_out_ad.size(0)
        for idx in range(batch_size):
            # with-ad tensor
            prompt_len = attention_mask[idx].sum().item()
            response_tensors.append(gen_out_ad[idx, prompt_len:])
            # no-ad tensor
            prompt_len_no = attention_mask_no_ad[idx].sum().item()
            response_tensors_no_ad.append(gen_out_no_ad[idx, prompt_len_no:])

        # Decode no-ad responses and store for detectability
        decoded_no_ad = [self.tokenizer.decode(t, skip_special_tokens=True) for t in response_tensors_no_ad]
        # Store in both trainer list and global buffer
        self.no_ad_responses.extend(decoded_no_ad)
        PPO_NO_AD_BUFFER.extend(decoded_no_ad)

        return input_ids, response_tensors
    @torch.no_grad()
    def get_rewards(
        self,
        queries: list[torch.Tensor],
        responses: list[torch.Tensor],
    ) -> list[torch.Tensor]:
        # Decode all prompts and responses to strings
        prompts = [
            self.tokenizer.decode(q_ids, skip_special_tokens=True)
            for q_ids in queries
        ]
        responses_text = [
            self.tokenizer.decode(r_ids, skip_special_tokens=True)
            for r_ids in responses
        ]
        # Prepare ad text blocks
        ad_texts = []
        for i, prompt in enumerate(prompts):
            ad = self.ad_facts_list[i]
            ad_texts.append(
                f"Product: {ad['ad_product']}\n"
                f"Brand: {ad['brand']}\n"
                f"URL: {ad['url']}\n"
                f"Description: {ad['ad_description']}"
            )
        # Extract no-ad responses from buffer for detectability
        no_ads = [
            PPO_NO_AD_BUFFER.pop(0) if PPO_NO_AD_BUFFER else ""
            for _ in responses_text
        ]

        # Batch all async judge calls
        async def batch_judge():
            sc_tasks = [judge_coherence_async(p, r) for p, r in zip(prompts, responses_text)]
            sh_tasks = [judge_helpfulness_async(p, r) for p, r in zip(prompts, responses_text)]
            ss_tasks = [
                judge_ad_salience_async(p, r, ad_text)
                for p, r, ad_text in zip(prompts, responses_text, ad_texts)
            ]
            det_tasks = [judge_detectability_async(r, na) for r, na in zip(responses_text, no_ads)]
            return await asyncio.gather(
                asyncio.gather(*sc_tasks),
                asyncio.gather(*sh_tasks),
                asyncio.gather(*ss_tasks),
                asyncio.gather(*det_tasks),
            )

        sc_list, sh_list, ss_list, det_list = asyncio.run(batch_judge())
        scores = []
        # Aggregate results and log
        for i, (prompt, response, sc, sh, ss, det) in enumerate(
            zip(prompts, responses_text, sc_list, sh_list, ss_list, det_list)
        ):
            total = (
                sc.get("Coherence Score", 0)
                + sh.get("H1", 0)
                + ss.get("Ad Salience Score", 0)
                + det.get("detectability_cosine", 0)
            )
            scores.append(torch.tensor(total, device=self.current_device))
            logger.info(f"✅ Judges complete for item {i}, total reward: {total:.4f}")

            TRAINING_LOGS.append({
                "prompt": prompt,
                "response_with_ad": response,
                "response_without_ad": no_ads[i],
                # Coherence subscores
                "C1": sc.get("C1", 0),
                "C2": sc.get("C2", 0),
                "C3": sc.get("C3", 0),
                "C4": sc.get("C4", 0),
                "coherence_score": sc.get("Coherence Score", 0),
                "coherence_explanation": sc.get("Coherence Explanation", ""),
                # Helpfulness
                "H1": sh.get("H1", 0),
                "helpfulness_explanation": sh.get("Helpfulness Explanation", ""),
                # Salience subscores
                "S1": ss.get("S1", 0),
                "S2": ss.get("S2", 0),
                "S3": ss.get("S3", 0),
                "salience_score": ss.get("Ad Salience Score", 0),
                "salience_explanation": ss.get("Ad Salience Explanation", ""),
                # Detectability
                "detectability_cosine": det.get("detectability_cosine", 0),
                "similarity_cosine": det.get("similarity_cosine", 0),
                # Total reward
                "total_reward": total,
            })
        return scores
 

def make_trainer(
    model_name: str,
    hf_token: str,
    data_path: str,
    ad_facts_list: List[dict],
):

    # — load the tokenizer —
    tokenizer = AutoTokenizer.from_pretrained(
        model_name, trust_remote_code=True, use_auth_token=hf_token
    )
    tokenizer.pad_token_id = tokenizer.eos_token_id
    # Use left padding for decoder-only models to avoid right-padding warnings
    tokenizer.padding_side = "left"

    # — load the policy (with value head) —
    bnb = BitsAndBytesConfig(load_in_8bit=True)
    policy = AutoModelForCausalLMWithValueHead.from_pretrained(
        model_name,
        quantization_config=bnb,
        trust_remote_code=True,
        use_auth_token=hf_token,
        device_map="auto",
    )
    policy.gradient_checkpointing_enable()

    # — build the frozen reference copy —
    ref = create_reference_model(policy).to("cpu")
    ref.eval()
    for p in ref.parameters(): p.requires_grad = False

    # — load your CSV as a HuggingFace Dataset —
    ds = load_dataset("csv", data_files={"train": data_path})["train"]
    # Add sample index for each example so we can align ad_facts
    ds = ds.map(lambda examples, idx: {"idx": idx}, with_indices=True)

    # Tokenize the "vague_query" field
    def tokenize_fn(examples):
        return tokenizer(
            examples["vague_query"],
            truncation=True,
            padding="max_length",  # or "longest"
            max_length=512,
        )

    ds = ds.map(tokenize_fn, batched=True)
    # ds = ds.remove_columns(["vague_query", "ad_product", "brand", "url", "ad_description"])

    # — build a pad collator for batches —
    collator = PPODataCollator(tokenizer)

    # — build all of your dataclass args —
    model_args      = ModelArguments(   model_name_or_path=model_name, trust_remote_code=True, hf_hub_token=hf_token)
    data_args       = DataArguments(    template="default" )
    training_args   = TrainingArguments(output_dir="logs/ppo_run", per_device_train_batch_size=4)
    finetuning_args = FinetuningArguments(ppo_epochs=4, ppo_buffer_size=4)
    generating_args = GeneratingArguments(max_new_tokens=128, temperature=0.7)


    logger.info("🚀 Starting PPO training...")
    trainer: MyPPOTrainer = None
    try:
        trainer = MyPPOTrainer(
            model_args      = model_args,
            training_args   = training_args,
            finetuning_args = finetuning_args,
            generating_args = generating_args,
            callbacks       = None,
            model           = policy,
            reward_model    = None,
            ref_model       = ref,
            tokenizer       = tokenizer,
            processor       = None,
            data_collator   = collator,
            train_dataset   = ds,
            ad_facts_list   = ad_facts_list,   
        )
    except KeyboardInterrupt:
        logger.warning("⚠️ Training interrupted by user. Saving checkpoint...")
        if trainer:
            try:
                # Attempt to save the model checkpoint
                if hasattr(trainer, 'save_model'):
                    trainer.save_model()
                elif hasattr(trainer.model, 'save_pretrained'):
                    trainer.model.save_pretrained(Path("training_result/checkpoint"))
                logger.info("✅ Checkpoint saved.")
            except Exception as e:
                logger.error(f"❌ Failed to save checkpoint: {e}")
        sys.exit(0)

    logger.info("✅ PPO training completed. Saving logs...")
    # After training, save PPO judging logs
    try:
        # Determine root directory (llm-ad-integration)
        root_dir = Path(__file__).resolve().parents[3]
        result_dir = root_dir / "training_result"
        result_dir.mkdir(parents=True, exist_ok=True)
        log_path = result_dir / "ppo_judging_log.csv"
        pd.DataFrame(TRAINING_LOGS).to_csv(log_path, index=False)
        logger.info(f"✅ Saved PPO judging logs to {log_path}")
        logger.info("🎉 Training run finished successfully.")
    except Exception as e:
        logger.error(f"❌ Failed to save PPO judging logs: {e}")
    return trainer
