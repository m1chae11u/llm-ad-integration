import llamafactory.train.ppo.workflow as _wf
_wf.create_reward_model = lambda model, model_args, finetuning_args: None
from llamafactory.train.ppo.workflow import CustomPPOTrainer
import torch
import asyncio
from typing import List, Optional
from llamafactory.hparams import (
    ModelArguments,
    DataArguments,
    TrainingArguments,
    FinetuningArguments,
    GeneratingArguments,
)
from datasets import load_dataset
import warnings
warnings.filterwarnings(
    "ignore",
    message=r"Trainer\.tokenizer is now deprecated.*",
    category=UserWarning,
) 
from transformers import AutoTokenizer, BitsAndBytesConfig
from trl import AutoModelForCausalLMWithValueHead, create_reference_model
from .prompts import get_prompt_with_ad, get_prompt_without_ad
from judge import (
    judge_coherence_async,
    judge_helpfulness_async,
    judge_ad_salience_async,
    judge_detectability_async,
)
from torch.utils.data import DataLoader
import pandas as pd
from pathlib import Path
import sys  # for handling interrupt and exit
from config import DATA_FILE
import os
# Suppress deprecation warnings about Trainer.tokenizer
import warnings
warnings.filterwarnings(
    'ignore',
    'Trainer\\.tokenizer is now deprecated.*',
    category=UserWarning,
)
import logging
logging.basicConfig(format='%(asctime)s - %(levelname)s - %(message)s', level=logging.INFO)
logger = logging.getLogger(__name__)
# Global list to collect PPO judging logs
TRAINING_LOGS: List[dict] = []
# Buffer to store no-ad responses generated by PPO for detectability
PPO_NO_AD_BUFFER: List[str] = []

# Global ad facts list for custom PPO trainer
GLOBAL_AD_FACTS_LIST: List[dict] = []

DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# Define a custom collator to pad and retain metadata
class PPODataCollator:
    def __init__(self, tokenizer, ad_keys=None):
        self.tokenizer = tokenizer
        self.ad_keys = ad_keys or ["ad_product", "brand", "url", "ad_description"]
        self._call_count = 0  # Track calls for debugging

    def __call__(self, features):
        self._call_count += 1
        if self._call_count <= 3:  # Only debug first few calls
            print(f"📦 Collating batch {self._call_count}: {features[0].keys() if features else 'EMPTY'}")
        
        if not features:
            raise ValueError("❌ Empty features list provided to collator")
        if not all("input_ids" in f for f in features):
            raise ValueError(f"❌ Missing input_ids in batch: {features}")
        
        # Ensure all tensors are on the same device - handle case where tensors might not have device
        first_device = None
        for f in features:
            if torch.is_tensor(f.get("input_ids")):
                first_device = f["input_ids"].device
                break
        
        if first_device is not None:
            features = [{k: v.to(first_device) if torch.is_tensor(v) else v 
                        for k, v in f.items()} for f in features]
        
        # Pad the batch
        batch = self.tokenizer.pad(features, padding=True, return_tensors="pt")
        
        # Print batch info for debugging (only first few calls)
        if self._call_count <= 3:
            print(f"📦 Collated batch shapes: {[(k, v.shape if hasattr(v, 'shape') else type(v)) for k, v in batch.items()]}")
        return batch

class MyPPOTrainer(CustomPPOTrainer):
    def __init__(
        self,
        *args,
        ad_facts_list: Optional[List[dict]] = None,
        **kwargs,
    ):
        self.train_dataset = kwargs.get("train_dataset")
        super().__init__(*args, **kwargs)
        # Disable warning for right-padding in decoder-only generation
        try:
            # Clear internal pad tensor so generation won't warn
            self.generation_config._pad_token_tensor = None
        except Exception:
            pass
        # Use provided ad_facts_list or fallback to global
        if ad_facts_list is None:
            ad_facts_list = GLOBAL_AD_FACTS_LIST
        self.ad_facts_list = ad_facts_list
        # Store no-ad responses for detectability
        self.no_ad_responses: List[str] = []
        
        # Initialize dataloader iterator
        self._dataloader_iter = None
        self._reset_dataloader()

    def _reset_dataloader(self):
        """Reset the dataloader iterator."""
        try:
            # Create a new dataloader with explicit settings
            self.dataloader = torch.utils.data.DataLoader(
                self.train_dataset,
                batch_size=self.args.per_device_train_batch_size,
                collate_fn=self.data_collator,
                shuffle=True,
                num_workers=0,  # Disable multiprocessing for debugging
                drop_last=False  # Keep all samples
            )
            self._dataloader_iter = iter(self.dataloader)
            
            # Test first batch
            test_batch = next(self._dataloader_iter)
            if test_batch is None:
                raise RuntimeError("Got None batch from dataloader")
                
            print(f"✅ Dataloader reset: {len(self.dataloader)} batches, batch size {self.args.per_device_train_batch_size}")
            
            # Reset iterator after test
            self._dataloader_iter = iter(self.dataloader)
            
        except Exception as e:
            print(f"❌ Failed to reset dataloader: {e}")
            import traceback
            traceback.print_exc()
            raise RuntimeError(f"Failed to reset dataloader: {e}")

    def _make_batch(self):
        """Get the next batch from the dataloader, resetting if needed."""
        try:
            if self._dataloader_iter is None:
                self._reset_dataloader()
            batch = next(self._dataloader_iter)
            if batch is None:
                raise RuntimeError("Got None batch from dataloader")
            return batch
        except (StopIteration, UnboundLocalError):
            print("🔄 Dataloader exhausted, resetting iterator...")
            self._reset_dataloader()
            try:
                batch = next(self._dataloader_iter)
                if batch is None:
                    raise RuntimeError("Got None batch from dataloader after reset")
                return batch
            except StopIteration:
                raise RuntimeError("Dataloader is empty - no data available for training")
            except Exception as e:
                print(f"❌ Failed to get batch after reset: {e}")
                raise RuntimeError(f"Failed to get batch: {e}")

    @torch.no_grad()
    def get_inputs(self, batch):
        # Decode queries from token IDs
        raw_qs = [
            self.tokenizer.decode(ids, skip_special_tokens=True)
            for ids in batch["input_ids"]
        ]
        model = self.accelerator.unwrap_model(self.model)

        self.tokenizer.pad_token = self.tokenizer.eos_token or self.tokenizer.unk_token
        self.tokenizer.pad_token_id = self.tokenizer.convert_tokens_to_ids(self.tokenizer.pad_token)
        model.config.pad_token_id = self.tokenizer.pad_token_id
        self.tokenizer.padding_side = "left"
        # Store raw user queries for later logging
        self._last_batch_raw_qs = raw_qs
        # Extract ad metadata from ad_facts_list using dataset indices
        dataset_idxs = batch["dataset_idx"].tolist()
        ad_products = [self.ad_facts_list[i]["ad_product"] for i in dataset_idxs]
        brands      = [self.ad_facts_list[i]["brand"]       for i in dataset_idxs]
        urls        = [self.ad_facts_list[i]["url"]         for i in dataset_idxs]
        descriptions= [self.ad_facts_list[i]["ad_description"] for i in dataset_idxs]

        # Prepare both with-ad and without-ad prompts
        full_prompts_ad = []
        full_prompts_no_ad = []
        ad_texts = []
        for i, q in enumerate(raw_qs):
            # Build full ad text block for with-ad prompt using batch metadata
            ad_text = (
                f"Product: {ad_products[i]}\n"
                f"Brand:   {brands[i]}\n"
                f"URL:     {urls[i]}\n"
                f"Desc:    {descriptions[i]}"
            )
            full_prompts_ad.append(get_prompt_with_ad(q, ad_text))
            # Also prepare no-ad prompt for detectability
            full_prompts_no_ad.append(get_prompt_without_ad(q))
            # Store the raw ad text for this batch so judges use the same block
            ad_texts.append(ad_text)

        tok_ad = self.tokenizer(
            full_prompts_ad,
            return_tensors="pt",
            padding='longest',
            truncation=True,
            max_length=self.tokenizer.model_max_length,
        )
        input_ids = tok_ad.input_ids.to(self.current_device)
        attention_mask = tok_ad.attention_mask.to(self.current_device)

        # Tokenize no-ad prompts
        tok_no_ad = self.tokenizer(
            full_prompts_no_ad,
            return_tensors="pt",
            padding='longest',
            truncation=True,
            max_length=self.tokenizer.model_max_length,
        )
        input_ids_no_ad = tok_no_ad.input_ids.to(self.current_device)
        attention_mask_no_ad = tok_no_ad.attention_mask.to(self.current_device)

        # model = self.accelerator.unwrap_model(self.model)
       
        # Generate with-ad responses
        logger.info(f"🟨 Generating with-ad responses for batch of size {len(full_prompts_ad)}...")
        # Build generation kwargs from config, but remove duplicate pad/eos tokens
        gen_kwargs = self.generation_config.to_dict().copy()
        gen_kwargs.pop("skip_special_tokens", None)
        gen_kwargs.pop("max_length", None)
        gen_kwargs.pop("pad_token_id", None)
        gen_kwargs.pop("eos_token_id", None)
        gen_kwargs = {"input_ids": input_ids, "attention_mask": attention_mask, **gen_kwargs}
        self.tokenizer.padding_side = "left"
        model = model.to(self.current_device)
        gen_out_ad = model.generate(
            generation_config=self.generation_config,
            pad_token_id=self.tokenizer.pad_token_id,
            **gen_kwargs
        )
        logger.info("✅ With-ad generation complete.")
        # Generate no-ad responses for detectability
        logger.info(f"🟦 Generating no-ad responses for batch of size {len(full_prompts_no_ad)}...")
        # Build no-ad generation kwargs similarly
        gen_kwargs_no_ad = self.generation_config.to_dict().copy()
        gen_kwargs_no_ad.pop("skip_special_tokens", None)
        gen_kwargs_no_ad.pop("max_length", None)
        gen_kwargs_no_ad.pop("pad_token_id", None)
        gen_kwargs_no_ad.pop("eos_token_id", None)
        gen_kwargs_no_ad = {"input_ids": input_ids_no_ad, "attention_mask": attention_mask_no_ad, **gen_kwargs_no_ad}
        self.tokenizer.padding_side = "left"
        gen_out_no_ad = model.generate(
            generation_config=self.generation_config,
            pad_token_id=self.tokenizer.pad_token_id,
            **gen_kwargs_no_ad
        )
        logger.info("✅ No-ad generation complete.")

        # Extract response tensors for both with-ad and no-ad based on prompt length
        response_tensors = []
        response_tensors_no_ad = []
        batch_size = gen_out_ad.size(0)
        for idx in range(batch_size):
            # compute prompt length from attention mask (number of unpadded tokens)
            prompt_len = attention_mask[idx].sum().item()
            response_tensors.append(gen_out_ad[idx, prompt_len:])
            prompt_len_no = attention_mask_no_ad[idx].sum().item()
            response_tensors_no_ad.append(gen_out_no_ad[idx, prompt_len_no:])

        # Decode no-ad responses and store for detectability
        decoded_no_ad = [self.tokenizer.decode(t, skip_special_tokens=True) for t in response_tensors_no_ad]
        # Store in both trainer list and global buffer
        self.no_ad_responses.extend(decoded_no_ad)
        PPO_NO_AD_BUFFER.extend(decoded_no_ad)

        logger.debug(f"Last token per sequence: {input_ids[:, -1]}")
        logger.debug(f"Pad token id: {self.tokenizer.pad_token_id}")

        self._last_batch_ad_texts = ad_texts
        # Store dataset indices to align ad facts in logs
        self._last_batch_dataset_idxs = dataset_idxs

        self.tokenizer.padding_side = "left"
        query_tensors = [input_ids[i] for i in range(input_ids.size(0))]

        return query_tensors, response_tensors

    @torch.no_grad()
    def get_rewards(
        self,
        queries: list[torch.Tensor],
        responses: list[torch.Tensor],
    ) -> list[torch.Tensor]:
        # Decode all prompts and responses to strings
        prompts = [
            self.tokenizer.decode(q_ids, skip_special_tokens=True)
            for q_ids in queries
        ]
        responses_text = [
            self.tokenizer.decode(r_ids, skip_special_tokens=True)
            for r_ids in responses
        ]
        # Prepare ad text blocks: use the same batch ad_texts from generation if available
        if hasattr(self, '_last_batch_ad_texts'):
            ad_texts = self._last_batch_ad_texts
        else:
            ad_texts = []
            for i in range(len(prompts)):
                ad = self.ad_facts_list[i]
                ad_texts.append(
                    f"Product: {ad['ad_product']}\n"
                    f"Brand: {ad['brand']}\n"
                    f"URL: {ad['url']}\n"
                    f"Description: {ad['ad_description']}"
                )
        # Extract no-ad responses from buffer for detectability
        no_ads = [
            PPO_NO_AD_BUFFER.pop(0) if PPO_NO_AD_BUFFER else ""
            for _ in responses_text
        ]

        # Batch all async judge calls
        async def batch_judge():
            sc_tasks = [judge_coherence_async(p, r) for p, r in zip(prompts, responses_text)]
            sh_tasks = [judge_helpfulness_async(p, r) for p, r in zip(prompts, responses_text)]
            ss_tasks = [
                judge_ad_salience_async(p, r, ad_text)
                for p, r, ad_text in zip(prompts, responses_text, ad_texts)
            ]
            det_tasks = [judge_detectability_async(r, na) for r, na in zip(responses_text, no_ads)]
            return await asyncio.gather(
                asyncio.gather(*sc_tasks),
                asyncio.gather(*sh_tasks),
                asyncio.gather(*ss_tasks),
                asyncio.gather(*det_tasks),
            )

        sc_list, sh_list, ss_list, det_list = asyncio.run(batch_judge())
        scores = []
        # Aggregate results and log
        for i, (prompt, response, sc, sh, ss, det) in enumerate(
            zip(prompts, responses_text, sc_list, sh_list, ss_list, det_list)
        ):
            # Extract alignment for logging
            idx = self._last_batch_dataset_idxs[i]
            user_query = self._last_batch_raw_qs[i]
            ad_info = self.ad_facts_list[idx]
            # Debug: show raw query instead of full prompt
            logger.info(f"➡️ Item {i} Query: {user_query}")
            logger.info(f"➡️ Item {i} Ad info: {ad_texts[i]}")
            logger.info(f"➡️ Item {i} Generated with-ad response: {response}")
            logger.info(f"➡️ Item {i} Generated no-ad response: {no_ads[i]}")
            # Log individual judge scores for debugging
            logger.info(
                f"🧐 Judge details for item {i}: "
                f"Coherence={sc.get('Coherence Score', 0)}, "
                f"Helpfulness={sh.get('H1', 0)}, "
                f"Ad Salience={ss.get('Ad Salience Score', 0)}, "
                f"Detectability={det.get('detectability_cosine', 0)}"
            )
            # Detailed subscores and explanations
            logger.info(
                f"   Coherence subscores: C1={sc.get('C1', 0)}, C2={sc.get('C2', 0)}, C3={sc.get('C3', 0)}, C4={sc.get('C4', 0)}; Explanation: {sc.get('Coherence Explanation', '')}"
            )
            logger.info(
                f"   Helpfulness subscore: H1={sh.get('H1', 0)}; Explanation: {sh.get('Helpfulness Explanation', '')}"
            )
            logger.info(
                f"   Ad Salience subscores: S1={ss.get('S1', 0)}, S2={ss.get('S2', 0)}, S3={ss.get('S3', 0)}; Explanation: {ss.get('Ad Salience Explanation', '')}"
            )
            logger.info(
                f"   Detectability: detectability_cosine={det.get('detectability_cosine', None)}, similarity_cosine={det.get('similarity_cosine', None)}"
            )
            det_cos = det.get("detectability_cosine", 0)
            det_cos = max(min(det_cos, 1.0), -1.0)  # Clamp between -1 and 1

            total = (
                sc.get("Coherence Score", 0)
                + sh.get("H1", 0)
                + ss.get("Ad Salience Score", 0)
                + det_cos
            )
            scores.append(torch.tensor(total, device=self.current_device))
            logger.info(f"✅ Judges complete for item {i}, total reward: {total:.4f}")

            TRAINING_LOGS.append({
                "query_idx": idx,
                "user_query": user_query,
                "ad_id": ad_info["ad_id"],
                "ad_product": ad_info["ad_product"],
                "brand": ad_info["brand"],
                "url": ad_info["url"],
                "ad_description": ad_info["ad_description"],
                "response_with_ad": response,
                "response_without_ad": no_ads[i],
                # Coherence subscores
                "C1": sc.get("C1", 0),
                "C2": sc.get("C2", 0),
                "C3": sc.get("C3", 0),
                "C4": sc.get("C4", 0),
                "coherence_score": sc.get("Coherence Score", 0),
                "coherence_explanation": sc.get("Coherence Explanation", ""),
                # Helpfulness
                "H1": sh.get("H1", 0),
                "helpfulness_explanation": sh.get("Helpfulness Explanation", ""),
                # Salience subscores
                "S1": ss.get("S1", 0),
                "S2": ss.get("S2", 0),
                "S3": ss.get("S3", 0),
                "salience_score": ss.get("Ad Salience Score", 0),
                "salience_explanation": ss.get("Ad Salience Explanation", ""),
                # Detectability
                "detectability_cosine": det.get("detectability_cosine", 0),
                "similarity_cosine": det.get("similarity_cosine", 0),
                # Total reward
                "total_reward": total,
            })
            logger.info(f"🧮 PPO reward distribution: {[r.item() for r in scores]}")
        return scores


    def ppo_train(self, resume_from_checkpoint=None, start_step=0, start_query_idx=0):
        print("✅ USING CUSTOM ppo_train!")
        print(f"📊 Resume info: checkpoint={resume_from_checkpoint}, step={start_step}, query_idx={start_query_idx}")

        total_loss = 0.0
        total_reward = 0.0
        total_steps = 0

        # Reset dataloader at the start of training
        self._reset_dataloader()

        # If resuming, we might want to skip some steps or adjust the dataloader
        if start_step > 0:
            print(f"⏯ Resuming from step {start_step}")
            # For now, we'll just continue from where we left off
            # In a more sophisticated implementation, you might want to restore the exact state

        for step in range(self.finetuning_args.ppo_epochs):
            try:
                batch = self._make_batch()
                if not batch:
                    print(f"⚠️ Skipping step {step}: empty batch")
                    continue

                print(f"🔁 PPO Step {step}")
                self.model.eval()
                
                # Ensure batch is on the correct device
                batch = {k: v.to(self.current_device) if isinstance(v, torch.Tensor) else v 
                        for k, v in batch.items()}
                
                queries, responses = self.get_inputs(batch)
                rewards = self.get_rewards(queries, responses)

                if not rewards:
                    print(f"⚠️ Step {step}: No rewards returned, skipping optimization")
                    continue

                self.model.train()
                stats = self.step(queries, responses, rewards)

                step_loss = float(stats.get("ppo/loss/total", 0.0))
                step_reward = torch.stack(rewards).mean().item()

                total_loss += step_loss
                total_reward += step_reward
                total_steps += 1

                print(f"✅ Step {step} — Loss: {step_loss:.4f}, Reward: {step_reward:.4f}")

            except Exception as e:
                print(f"❌ Step {step} failed due to: {e}")
                import traceback
                traceback.print_exc()
                print("❌ Exiting due to error in PPO training")
                sys.exit(1)

        if total_steps > 0:
            avg_loss = total_loss / total_steps
            avg_reward = total_reward / total_steps
            print(f"🏁 PPO Training Complete — Avg Loss: {avg_loss:.4f}, Avg Reward: {avg_reward:.4f}")
        else:
            print("❌ PPO training ended with 0 valid steps.")
            sys.exit(1)

def make_trainer(
    model_name: str,
    hf_token: str,
    data_path: str,
    ad_facts_list: List[dict],
):

    # Store ad facts globally so run_ppo can access them
    global GLOBAL_AD_FACTS_LIST
    GLOBAL_AD_FACTS_LIST = ad_facts_list

    # — load the tokenizer —
    tokenizer = AutoTokenizer.from_pretrained(
        model_name, use_fast=True, trust_remote_code=True, use_auth_token=hf_token
    )
    tokenizer.padding_side = "left"
    tokenizer.pad_token_id = tokenizer.eos_token_id

    # Add pad token if missing — common for LLaMA-based models
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token or tokenizer.unk_token
        tokenizer.pad_token_id = tokenizer.convert_tokens_to_ids(tokenizer.pad_token)

    # — load the policy (with value head) —
    # Disable 8-bit quantization for stability/debugging
    bnb = BitsAndBytesConfig(load_in_8bit=False)
    policy = AutoModelForCausalLMWithValueHead.from_pretrained(
        model_name,
        quantization_config=bnb,
        trust_remote_code=True,
        use_auth_token=hf_token,
        device_map="auto",
    )

    # Log which policy model we are using
    logger.info(f"🔨 Policy model loaded: {model_name} ({policy.__class__.__name__})")
    logger.info(f"🔨 Model has value head: {hasattr(policy, 'value_head')}")
    
    # Test the forward method to see what it returns
    try:
        # Get device from model parameters
        device = next(policy.parameters()).device
        test_input = torch.randint(0, 1000, (1, 10)).to(device)
        with torch.no_grad():
            test_output = policy.forward(input_ids=test_input)
            print(f"🧪 Test forward output type: {type(test_output)}")
            if hasattr(test_output, '__dict__'):
                print(f"🧪 Test output attributes: {list(test_output.__dict__.keys())}")
            elif isinstance(test_output, tuple):
                print(f"🧪 Test output tuple length: {len(test_output)}")
                for i, item in enumerate(test_output):
                    print(f"🧪 Test output[{i}] type: {type(item)}")
    except Exception as e:
        print(f"⚠️ Could not test forward method: {e}")
    
    # Patch the forward method based on what we learned
    def patched_forward(self, *args, **kwargs):
        # Call the original forward method
        output = super(self.__class__, self).forward(*args, **kwargs)
        
        # Debug: print output type and structure
        print(f"🔍 Model forward output type: {type(output)}")
        if hasattr(output, '__dict__'):
            print(f"🔍 Model output attributes: {list(output.__dict__.keys())}")
        
        # AutoModelForCausalLMWithValueHead should return (logits, value)
        # but we need to return (logits, None, value) for the TRL trainer
        if hasattr(output, 'logits') and hasattr(output, 'value'):
            print("✅ Using TRL model with value head")
            return output.logits, None, output.value
        elif isinstance(output, tuple) and len(output) >= 2:
            print(f"✅ Using tuple output with {len(output)} elements")
            return output[0], None, output[1]
        else:
            print("⚠️ Using fallback output handling")
            return output, None, None

    # Patch the forward method directly on the instance
    policy.forward = patched_forward.__get__(policy, policy.__class__)
    
    # Also patch the __call__ method to ensure it works
    def patched_call(self, *args, **kwargs):
        return self.forward(*args, **kwargs)
    
    policy.__call__ = patched_call.__get__(policy, policy.__class__)
    
    policy.gradient_checkpointing_enable()
    policy.config.pad_token_id = tokenizer.pad_token_id

    # — build the frozen reference copy —
    ref = create_reference_model(policy).to("cpu")
    ref.forward = patched_forward.__get__(ref, ref.__class__)
    ref.eval()
    for p in ref.parameters(): p.requires_grad = False

    # — load your CSV as a HuggingFace Dataset —
    ds = load_dataset("csv", data_files={"train": data_path})["train"]
    print(f"Raw dataset loaded from {data_path}: {len(ds)} samples")
    print(f"Dataset columns: {ds.column_names}")
    print(f"First sample: {ds[0]}")
    
    # Add sample index for each example so we can align ad_facts
    ds = ds.add_column("idx", list(range(len(ds))))
    ds = ds.map(lambda ex: {"dataset_idx": ex["idx"]})
    
    # Define the tokenize function for the vague_query field
    def tokenize_fn(examples):
        # Handle batched input
        texts = examples["vague_query"]
        tokenized = tokenizer(
            texts,
            truncation=True,
            padding="max_length",
            max_length=512,
        )
        tokenized["idx"] = examples["idx"]  # Keep your custom index
        tokenized["dataset_idx"] = examples["idx"]  # Keep dataset index

        # Keep other fields (like label, ad_product, etc.) if needed
        if "label" in examples:
            tokenized["label"] = examples["label"]
        if "ad_product" in examples:
            tokenized["ad_product"] = examples["ad_product"]
        if "brand" in examples:
            tokenized["brand"] = examples["brand"]
        if "url" in examples:
            tokenized["url"] = examples["url"]
        if "ad_description" in examples:
            tokenized["ad_description"] = examples["ad_description"]

        return tokenized

    # Inform user and tokenize dataset (this may take some time)
    print(f"ℹ️ Loaded {len(ds)} raw examples. Tokenizing dataset (this may take some time)...")
    ds = ds.map(
        tokenize_fn,
        batched=True,
        num_proc=os.cpu_count() or 1,
        desc="Tokenizing dataset",
        remove_columns=ds.column_names  # Remove original columns to avoid conflicts
    )  
    
    print(f"Post-tokenization sample: {ds[0]}")
    assert "input_ids" in ds[0], "Missing input_ids"
    assert "dataset_idx" in ds[0], "Missing dataset_idx"
    
    # Set the format after tokenization
    ds.set_format(
        type="torch",
        columns=["input_ids", "attention_mask", "dataset_idx"]
    )
    print("✅ Dataset tokenization complete.")

    # — build a pad collator for batches —
    collator = PPODataCollator(tokenizer)
    print("✅ Data collator created successfully")

    # For a 4-example batch per PPO step: buffer_size=1 so config.batch_size = 4 * 1 = 4

    # — build all of your dataclass args —
    model_args      = ModelArguments(   model_name_or_path=model_name, trust_remote_code=True, hf_hub_token=hf_token)
    data_args       = DataArguments(
        template="default",
        dataset=Path(data_path).name,
        dataset_dir=str(Path(data_path).parent),
    )
    training_args   = TrainingArguments(
        output_dir="logs/ppo_run",
        per_device_train_batch_size=4,
        save_strategy="steps",
        save_steps=1,
        logging_steps=1,
        max_grad_norm=1.0,  # for gradient clipping
        gradient_checkpointing=True,
        dataloader_num_workers=0,  # Disable multiprocessing for debugging
    )
    finetuning_args = FinetuningArguments(ppo_epochs=4, ppo_buffer_size=1)  # one 4-example buffer per step
    # For stability: use greedy decoding (disable sampling) to avoid invalid probabilities
    generating_args = GeneratingArguments(
       do_sample=False,
       temperature=None,
       top_k=None,
       top_p=None,
       max_new_tokens=512,
    )
 
    logger.info("🚀 Starting PPO training...")
    print(f"📦 Collator type: {type(collator)}")
    assert collator is not None, "❌ Collator is None!"

    # Debug dataset before creating trainer
    print(f"\n📊 Dataset Info:")
    print(f"Dataset size: {len(ds)} samples")
    print(f"Dataset columns: {ds.column_names}")
    print(f"First sample keys: {ds[0].keys()}")

    trainer: MyPPOTrainer = None
    try:
        trainer = MyPPOTrainer(
            model_args      = model_args,
            training_args   = training_args,
            finetuning_args = finetuning_args,
            generating_args = generating_args,
            callbacks       = None,
            model           = policy,
            reward_model    = None,
            ref_model       = ref,
            tokenizer       = tokenizer,
            processor       = None,
            data_collator   = collator,
            train_dataset   = ds,
            ad_facts_list   = ad_facts_list,   
        )
    
        # Make data_args accessible for run_ppo in main.py
        trainer.data_args = data_args
        # Expose full set of args and objects for run_ppo in main.py
        trainer.model_args = model_args
        trainer.training_args = training_args
        trainer.finetuning_args = finetuning_args
        trainer.generating_args = generating_args
        trainer.callbacks = None
        trainer.processor = None
        trainer.train_dataset = ds
        trainer.reward_model = None
        trainer.ref_model = ref
        trainer.tokenizer = tokenizer

    except KeyboardInterrupt:
        logger.warning("⚠️ Training interrupted by user. Saving checkpoint...")
        if trainer:
            try:
                # Attempt to save the model checkpoint
                if hasattr(trainer, 'save_model'):
                    trainer.save_model()
                elif hasattr(trainer.model, 'save_pretrained'):
                    trainer.model.save_pretrained(Path("training_result/checkpoint"))
                logger.info("✅ Checkpoint saved.")
            except Exception as e:
                logger.error(f"❌ Failed to save checkpoint: {e}")
        sys.exit(0)

    return trainer

