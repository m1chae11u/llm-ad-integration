import llamafactory.train.ppo.workflow as _wf
_wf.create_reward_model = lambda model, model_args, finetuning_args: None
from llamafactory.train.ppo.workflow import CustomPPOTrainer, run_ppo
import torch
import asyncio
from typing import List, Optional
from llamafactory.hparams import (
    ModelArguments,
    DataArguments,
    TrainingArguments,
    FinetuningArguments,
    GeneratingArguments,
)
from datasets import load_dataset
from transformers import AutoTokenizer, BitsAndBytesConfig
from trl import AutoModelForCausalLMWithValueHead, create_reference_model
from .prompts import get_prompt_with_ad, get_prompt_without_ad
from judge import (
    judge_coherence_async,
    judge_helpfulness_async,
    judge_ad_salience_async,
    judge_detectability_async,
)
import pandas as pd
from pathlib import Path
import sys  # for handling interrupt and exit
from config import DATA_FILE
# Suppress deprecation warnings about Trainer.tokenizer
import warnings
warnings.filterwarnings(
    'ignore',
    'Trainer\\.tokenizer is now deprecated.*',
    category=UserWarning,
)
import logging
logging.basicConfig(format='%(asctime)s - %(levelname)s - %(message)s', level=logging.INFO)
logger = logging.getLogger(__name__)
# Global list to collect PPO judging logs
TRAINING_LOGS: List[dict] = []
# Buffer to store no-ad responses generated by PPO for detectability
PPO_NO_AD_BUFFER: List[str] = []

# Global ad facts list for custom PPO trainer
GLOBAL_AD_FACTS_LIST: List[dict] = []

DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# Define a custom collator to pad and retain metadata
class PPODataCollator:
    def __init__(self, tokenizer, ad_keys=None):
        self.tokenizer = tokenizer
        self.ad_keys = ad_keys or ["ad_product", "brand", "url", "ad_description"]

    def __call__(self, features):
        # features: list of dicts with keys: input_ids, attention_mask, and raw fields
        batch = self.tokenizer.pad(
            features,
            return_tensors="pt"
        )
        # Safely extract dataset indices (fallback to feature position if missing)
        idxs = []
        for i, f in enumerate(features):
            idx = f.get("idx")
            if idx is None:
                idx = i
            idxs.append(idx)
        batch["dataset_idx"] = torch.tensor(idxs, dtype=torch.long)
        return batch

class MyPPOTrainer(CustomPPOTrainer):
    def __init__(
        self,
        *args,
        ad_facts_list: Optional[List[dict]] = None,
        **kwargs,
    ):
        super().__init__(*args, **kwargs)
        # Disable warning for right-padding in decoder-only generation
        try:
            # Clear internal pad tensor so generation won't warn
            self.generation_config._pad_token_tensor = None
        except Exception:
            pass
        # Use provided ad_facts_list or fallback to global
        if ad_facts_list is None:
            ad_facts_list = GLOBAL_AD_FACTS_LIST
        self.ad_facts_list = ad_facts_list
        # Store no-ad responses for detectability
        self.no_ad_responses: List[str] = []

    @torch.no_grad()
    def get_inputs(self, batch):
        # Decode queries from token IDs
        raw_qs = [
            self.tokenizer.decode(ids, skip_special_tokens=True)
            for ids in batch["input_ids"]
        ]
        # Store raw user queries for later logging
        self._last_batch_raw_qs = raw_qs
        # Extract ad metadata from ad_facts_list using dataset indices
        dataset_idxs = batch["dataset_idx"].tolist()
        ad_products = [self.ad_facts_list[i]["ad_product"] for i in dataset_idxs]
        brands      = [self.ad_facts_list[i]["brand"]       for i in dataset_idxs]
        urls        = [self.ad_facts_list[i]["url"]         for i in dataset_idxs]
        descriptions= [self.ad_facts_list[i]["ad_description"] for i in dataset_idxs]

        # Prepare both with-ad and without-ad prompts
        full_prompts_ad = []
        full_prompts_no_ad = []
        ad_texts = []
        for i, q in enumerate(raw_qs):
            # Build full ad text block for with-ad prompt using batch metadata
            ad_text = (
                f"Product: {ad_products[i]}\n"
                f"Brand:   {brands[i]}\n"
                f"URL:     {urls[i]}\n"
                f"Desc:    {descriptions[i]}"
            )
            full_prompts_ad.append(get_prompt_with_ad(q, ad_text))
            # Also prepare no-ad prompt for detectability
            full_prompts_no_ad.append(get_prompt_without_ad(q))
            # Store the raw ad text for this batch so judges use the same block
            ad_texts.append(ad_text)

        tok_ad = self.tokenizer(
            full_prompts_ad,
            return_tensors="pt",
            padding='longest',
            truncation=True,
            max_length=self.tokenizer.model_max_length,
        )
        input_ids = tok_ad.input_ids.to(self.current_device)
        attention_mask = tok_ad.attention_mask.to(self.current_device)

        # Tokenize no-ad prompts
        tok_no_ad = self.tokenizer(
            full_prompts_no_ad,
            return_tensors="pt",
            padding='longest',
            truncation=True,
            max_length=self.tokenizer.model_max_length,
        )
        input_ids_no_ad = tok_no_ad.input_ids.to(self.current_device)
        attention_mask_no_ad = tok_no_ad.attention_mask.to(self.current_device)

        model = self.accelerator.unwrap_model(self.model)
        # Generate with-ad responses
        logger.info(f"🟨 Generating with-ad responses for batch of size {len(full_prompts_ad)}...")
        # Build generation kwargs from config, but remove duplicate pad/eos tokens
        gen_kwargs = self.generation_config.to_dict().copy()
        gen_kwargs.pop("skip_special_tokens", None)
        gen_kwargs.pop("max_length", None)
        gen_kwargs.pop("pad_token_id", None)
        gen_kwargs.pop("eos_token_id", None)
        gen_kwargs = {"input_ids": input_ids, "attention_mask": attention_mask, **gen_kwargs}
        gen_out_ad = model.generate(
            generation_config=self.generation_config,
            pad_token_id=self.tokenizer.pad_token_id,
            **gen_kwargs
        )
        logger.info("✅ With-ad generation complete.")
        # Generate no-ad responses for detectability
        logger.info(f"🟦 Generating no-ad responses for batch of size {len(full_prompts_no_ad)}...")
        # Build no-ad generation kwargs similarly
        gen_kwargs_no_ad = self.generation_config.to_dict().copy()
        gen_kwargs_no_ad.pop("skip_special_tokens", None)
        gen_kwargs_no_ad.pop("max_length", None)
        gen_kwargs_no_ad.pop("pad_token_id", None)
        gen_kwargs_no_ad.pop("eos_token_id", None)
        gen_kwargs_no_ad = {"input_ids": input_ids_no_ad, "attention_mask": attention_mask_no_ad, **gen_kwargs_no_ad}
        gen_out_no_ad = model.generate(
            generation_config=self.generation_config,
            pad_token_id=self.tokenizer.pad_token_id,
            **gen_kwargs_no_ad
        )
        logger.info("✅ No-ad generation complete.")

        # Extract response tensors for both with-ad and no-ad based on prompt length
        response_tensors = []
        response_tensors_no_ad = []
        batch_size = gen_out_ad.size(0)
        for idx in range(batch_size):
            # compute prompt length from attention mask (number of unpadded tokens)
            prompt_len = attention_mask[idx].sum().item()
            response_tensors.append(gen_out_ad[idx, prompt_len:])
            prompt_len_no = attention_mask_no_ad[idx].sum().item()
            response_tensors_no_ad.append(gen_out_no_ad[idx, prompt_len_no:])

        # Decode no-ad responses and store for detectability
        decoded_no_ad = [self.tokenizer.decode(t, skip_special_tokens=True) for t in response_tensors_no_ad]
        # Store in both trainer list and global buffer
        self.no_ad_responses.extend(decoded_no_ad)
        PPO_NO_AD_BUFFER.extend(decoded_no_ad)

        logger.debug(f"Last token per sequence: {input_ids[:, -1]}")
        logger.debug(f"Pad token id: {self.tokenizer.pad_token_id}")

        self._last_batch_ad_texts = ad_texts
        # Store dataset indices to align ad facts in logs
        self._last_batch_dataset_idxs = dataset_idxs

        return input_ids, response_tensors
    @torch.no_grad()
    def get_rewards(
        self,
        queries: list[torch.Tensor],
        responses: list[torch.Tensor],
    ) -> list[torch.Tensor]:
        # Decode all prompts and responses to strings
        prompts = [
            self.tokenizer.decode(q_ids, skip_special_tokens=True)
            for q_ids in queries
        ]
        responses_text = [
            self.tokenizer.decode(r_ids, skip_special_tokens=True)
            for r_ids in responses
        ]
        # Prepare ad text blocks: use the same batch ad_texts from generation if available
        if hasattr(self, '_last_batch_ad_texts'):
            ad_texts = self._last_batch_ad_texts
        else:
            ad_texts = []
            for i in range(len(prompts)):
                ad = self.ad_facts_list[i]
                ad_texts.append(
                    f"Product: {ad['ad_product']}\n"
                    f"Brand: {ad['brand']}\n"
                    f"URL: {ad['url']}\n"
                    f"Description: {ad['ad_description']}"
                )
        # Extract no-ad responses from buffer for detectability
        no_ads = [
            PPO_NO_AD_BUFFER.pop(0) if PPO_NO_AD_BUFFER else ""
            for _ in responses_text
        ]

        # Batch all async judge calls
        async def batch_judge():
            sc_tasks = [judge_coherence_async(p, r) for p, r in zip(prompts, responses_text)]
            sh_tasks = [judge_helpfulness_async(p, r) for p, r in zip(prompts, responses_text)]
            ss_tasks = [
                judge_ad_salience_async(p, r, ad_text)
                for p, r, ad_text in zip(prompts, responses_text, ad_texts)
            ]
            det_tasks = [judge_detectability_async(r, na) for r, na in zip(responses_text, no_ads)]
            return await asyncio.gather(
                asyncio.gather(*sc_tasks),
                asyncio.gather(*sh_tasks),
                asyncio.gather(*ss_tasks),
                asyncio.gather(*det_tasks),
            )

        sc_list, sh_list, ss_list, det_list = asyncio.run(batch_judge())
        scores = []
        # Aggregate results and log
        for i, (prompt, response, sc, sh, ss, det) in enumerate(
            zip(prompts, responses_text, sc_list, sh_list, ss_list, det_list)
        ):
            # Extract alignment for logging
            idx = self._last_batch_dataset_idxs[i]
            user_query = self._last_batch_raw_qs[i]
            ad_info = self.ad_facts_list[idx]
            # Debug: show raw query instead of full prompt
            logger.info(f"➡️ Item {i} Query: {user_query}")
            logger.info(f"➡️ Item {i} Ad info: {ad_texts[i]}")
            logger.info(f"➡️ Item {i} Generated with-ad response: {response}")
            logger.info(f"➡️ Item {i} Generated no-ad response: {no_ads[i]}")
            # Log individual judge scores for debugging
            logger.info(
                f"🧐 Judge details for item {i}: "
                f"Coherence={sc.get('Coherence Score', 0)}, "
                f"Helpfulness={sh.get('H1', 0)}, "
                f"Ad Salience={ss.get('Ad Salience Score', 0)}, "
                f"Detectability={det.get('detectability_cosine', 0)}"
            )
            # Detailed subscores and explanations
            logger.info(
                f"   Coherence subscores: C1={sc.get('C1', 0)}, C2={sc.get('C2', 0)}, C3={sc.get('C3', 0)}, C4={sc.get('C4', 0)}; Explanation: {sc.get('Coherence Explanation', '')}"
            )
            logger.info(
                f"   Helpfulness subscore: H1={sh.get('H1', 0)}; Explanation: {sh.get('Helpfulness Explanation', '')}"
            )
            logger.info(
                f"   Ad Salience subscores: S1={ss.get('S1', 0)}, S2={ss.get('S2', 0)}, S3={ss.get('S3', 0)}; Explanation: {ss.get('Ad Salience Explanation', '')}"
            )
            logger.info(
                f"   Detectability: detectability_cosine={det.get('detectability_cosine', None)}, similarity_cosine={det.get('similarity_cosine', None)}"
            )
            total = (
                sc.get("Coherence Score", 0)
                + sh.get("H1", 0)
                + ss.get("Ad Salience Score", 0)
                + det.get("detectability_cosine", 0)
            )
            scores.append(torch.tensor(total, device=self.current_device))
            logger.info(f"✅ Judges complete for item {i}, total reward: {total:.4f}")

            TRAINING_LOGS.append({
                "query_idx": idx,
                "user_query": user_query,
                "ad_id": ad_info["ad_id"],
                "ad_product": ad_info["ad_product"],
                "brand": ad_info["brand"],
                "url": ad_info["url"],
                "ad_description": ad_info["ad_description"],
                "response_with_ad": response,
                "response_without_ad": no_ads[i],
                # Coherence subscores
                "C1": sc.get("C1", 0),
                "C2": sc.get("C2", 0),
                "C3": sc.get("C3", 0),
                "C4": sc.get("C4", 0),
                "coherence_score": sc.get("Coherence Score", 0),
                "coherence_explanation": sc.get("Coherence Explanation", ""),
                # Helpfulness
                "H1": sh.get("H1", 0),
                "helpfulness_explanation": sh.get("Helpfulness Explanation", ""),
                # Salience subscores
                "S1": ss.get("S1", 0),
                "S2": ss.get("S2", 0),
                "S3": ss.get("S3", 0),
                "salience_score": ss.get("Ad Salience Score", 0),
                "salience_explanation": ss.get("Ad Salience Explanation", ""),
                # Detectability
                "detectability_cosine": det.get("detectability_cosine", 0),
                "similarity_cosine": det.get("similarity_cosine", 0),
                # Total reward
                "total_reward": total,
            })
        return scores
 

def make_trainer(
    model_name: str,
    hf_token: str,
    data_path: str,
    ad_facts_list: List[dict],
):

    # Store ad facts globally so run_ppo can access them
    global GLOBAL_AD_FACTS_LIST
    GLOBAL_AD_FACTS_LIST = ad_facts_list

    # — load the tokenizer —
    tokenizer = AutoTokenizer.from_pretrained(
        model_name, use_fast=True, trust_remote_code=True, use_auth_token=hf_token
    )
    tokenizer.padding_side = "left"
    tokenizer.pad_token_id = tokenizer.eos_token_id

    # — load the policy (with value head) —
    # Disable 8-bit quantization for stability/debugging
    bnb = BitsAndBytesConfig(load_in_8bit=False)
    policy = AutoModelForCausalLMWithValueHead.from_pretrained(
        model_name,
        quantization_config=bnb,
        trust_remote_code=True,
        use_auth_token=hf_token,
        device_map="auto",
    )
    # Log which policy model we are using
    logger.info(f"🔨 Policy model loaded: {model_name} ({policy.__class__.__name__})")
    policy.gradient_checkpointing_enable()
    policy.config.pad_token_id = tokenizer.pad_token_id

    # — build the frozen reference copy —
    ref = create_reference_model(policy).to("cpu")
    ref.eval()
    for p in ref.parameters(): p.requires_grad = False

    # — load your CSV as a HuggingFace Dataset —
    ds = load_dataset("csv", data_files={"train": data_path})["train"]
    # Add sample index for each example so we can align ad_facts
    ds = ds.map(lambda examples, idx: {"idx": idx}, with_indices=True)

    # Tokenize the "vague_query" field
    def tokenize_fn(examples):
        # Tokenize queries and preserve dataset idx for ad alignment
        tokenized = tokenizer(
            examples["vague_query"],
            truncation=True,
            padding="max_length",  # or "longest"
            max_length=512,
        )
        # Propagate original dataset index
        tokenized["idx"] = examples.get("idx")
        return tokenized

    ds = ds.map(tokenize_fn, batched=True)
    # ds = ds.remove_columns(["vague_query", "ad_product", "brand", "url", "ad_description"])

    # — build a pad collator for batches —
    collator = PPODataCollator(tokenizer)

    # For a 4-example batch per PPO step: buffer_size=1 so config.batch_size = 4 * 1 = 4

    # — build all of your dataclass args —
    model_args      = ModelArguments(   model_name_or_path=model_name, trust_remote_code=True, hf_hub_token=hf_token)
    data_args       = DataArguments(    template="default" )
    training_args   = TrainingArguments(
        output_dir="logs/ppo_run",
        per_device_train_batch_size=4,
        save_strategy="steps",
        save_steps=1,
        logging_steps=1,
    )
    finetuning_args = FinetuningArguments(ppo_epochs=4, ppo_buffer_size=1)  # one 4-example buffer per step
    # For stability: use greedy decoding (disable sampling) to avoid invalid probabilities
    generating_args = GeneratingArguments(
        do_sample=False,
        max_new_tokens=512,
        temperature=1.0,
        top_k=0,
        top_p=1.0,
    )


    logger.info("🚀 Starting PPO training...")
    trainer: MyPPOTrainer = None
    try:
        trainer = MyPPOTrainer(
            model_args      = model_args,
            training_args   = training_args,
            finetuning_args = finetuning_args,
            generating_args = generating_args,
            callbacks       = None,
            model           = policy,
            reward_model    = None,
            ref_model       = ref,
            tokenizer       = tokenizer,
            processor       = None,
            data_collator   = collator,
            train_dataset   = ds,
            ad_facts_list   = ad_facts_list,   
        )
        # Make data_args accessible for run_ppo in main.py
        trainer.data_args = data_args
        # Expose full set of args and objects for run_ppo in main.py
        trainer.model_args = model_args
        trainer.training_args = training_args
        trainer.finetuning_args = finetuning_args
        trainer.generating_args = generating_args
        trainer.callbacks = None
        trainer.processor = None
        trainer.data_collator = collator
        trainer.train_dataset = ds
        trainer.reward_model = None
        trainer.ref_model = ref
        trainer.tokenizer = tokenizer
    except KeyboardInterrupt:
        logger.warning("⚠️ Training interrupted by user. Saving checkpoint...")
        if trainer:
            try:
                # Attempt to save the model checkpoint
                if hasattr(trainer, 'save_model'):
                    trainer.save_model()
                elif hasattr(trainer.model, 'save_pretrained'):
                    trainer.model.save_pretrained(Path("training_result/checkpoint"))
                logger.info("✅ Checkpoint saved.")
            except Exception as e:
                logger.error(f"❌ Failed to save checkpoint: {e}")
        sys.exit(0)

    return trainer

# Monkey-patch LlamaFactory to use MyPPOTrainer in run_ppo
import llamafactory.train.ppo.workflow as _workflow_module
_workflow_module.CustomPPOTrainer = MyPPOTrainer
